{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi_kn2hJujK3"
      },
      "source": [
        "# DQN on CartPole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWBf2DBSSoZN",
        "outputId": "468af54f-73ea-4735-e95c-d3558f226725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAYcxgdkUk1s"
      },
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v0')\n",
        "for i_episode in range(10):\n",
        "    next_state = env.reset()\n",
        "    for t in range(100):\n",
        "        #env.render()\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        #print(t, next_state, done, info, action)\n",
        "        if done:\n",
        "            break\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkK4zV8JX13P"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "The objective is to balance a pole vertically. This is done by moving a cart upon which the pole is standing left to right. Success is deemed by the pole staying vertical for more than 500 frames while failure is deemed by the pole reaching an angle greater than 50 degrees from the vertical position or when the cart is more than 2.4 units from the centre. Every time we reach the vertical position our reward goes up by one; therefore the goal is to reach a reward of 500."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxEzm3-YrXg"
      },
      "source": [
        "I chose to implemetn a DQN etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WuoV8umaVcc"
      },
      "source": [
        "CartPole is based on a Markov model. etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReaMqUnIUont"
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.metrics import Accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "verYUGI6fF-2"
      },
      "source": [
        "##Neural Network\n",
        "This model uses a neural network that learns on the example input and output pairs. It detects some kind of pattern and builds a predicitve model for unseen inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-sphcQwco3Z"
      },
      "source": [
        "def nnModel(input_shape, action_space):\n",
        "\n",
        "  x_input = Input(input_shape)\n",
        "\n",
        "  # Dense is the basic form of a neural network layer\n",
        "  # Input layer of state size 4 and Hidden Layer with 512 nodes\n",
        "  x = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(x_input)\n",
        "\n",
        "  # Hidden Layer with 256 nodes\n",
        "  x = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
        "\n",
        "  # Hidden Layer with 64 nodes\n",
        "  x = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
        "\n",
        "  # Output Layer with # of actions: 2 nodes (left, right)\n",
        "  x = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(x)\n",
        "\n",
        "  model = Model(inputs= x_input, outputs = x, name='CartPole DQN model')\n",
        "  model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiUZ7sHGflF9"
      },
      "source": [
        "Our loss function is defined as\n",
        "\n",
        "$loss=(r+\\gamma maxQ'(s,a')-Q(s,a))^2$\n",
        "\n",
        "We carry out an action a, observe the reward r and obtain a new state s. The result is used to calculate the target Q and then discount it so that the future reward is worth less than the immediate reward. Adding the current reward to the discounted future reward gives us our target value. Subtracting the current prediction from the target results in the loss. This is squared so that large loss is disencouraged and also so that negative values are treated the same as positive ones.\n",
        "\n",
        "The target is defined by:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        " target = reward + gamma * np.max(model.predict(next_state))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z99rIguihwiV"
      },
      "source": [
        "## Memory Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30yO_BQ9vTIq"
      },
      "source": [
        "In a DQN, the neural network collects experiences as outputs predictions. These experiences can be stored in a list and then can be sampled during training to update the Q value. This is experience replay. The memory function appends the information to the memory list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBEjzzh6fAFE"
      },
      "source": [
        "def memory(self, state, action, reward, next_state, done):\n",
        "  self.memory.append((self, state, action, reward, next_state, done))\n",
        "  if len(self.memory) > self.train_start:\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      # Discount epsilon to maximize the discounted future reward\n",
        "      self.epsilon *= self.epsilon_decay "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGCmhHs_w_AP"
      },
      "source": [
        "## Replay Function\n",
        "\n",
        "This function trains the neural network using experiences from the memory. Experiences are sampled in mini batches which are selection of randomly sampled memories from the total batch size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgx_omPUw9b2"
      },
      "source": [
        "def replay(self):\n",
        "  if len(self.memory) < self.train_start:\n",
        "    return\n",
        "\n",
        "  # randomly sample minibatch from the memory\n",
        "  minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
        "\n",
        "  state = np.zeros((self.batch_size, self.state_size))\n",
        "  next_state = np.zeros((self.batch_size, self.state_size))\n",
        "  action, reward, done = [], [], []\n",
        "\n",
        "  # this can be done using tensors for faster computational time\n",
        "  for i in rnage(self.batch_size):\n",
        "    state[i] = minibatch[i][0]\n",
        "    action.append(minibatch[i][1])\n",
        "    reward.append(minibatch[i][2])\n",
        "    next_state[i] = minibatch[i][3]\n",
        "    done.append(minibatch[i][4])\n",
        "\n",
        "  # do batch prediction to save speed\n",
        "  target = self.model.predict(state)\n",
        "  target_next = self.model.predict(next_state)\n",
        "\n",
        "  for i in range(self.batch_size):\n",
        "    if done[i]:\n",
        "      target[i][action[i]] = reward[i]\n",
        "    else:\n",
        "      # DQN chooses the max Q value among next actions\n",
        "      # Q_max = max_a' Q_target(s', a')\n",
        "      target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
        "      \n",
        "    # Train the Neural Network\n",
        "    self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTWO8g2-0yTh"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgmC_vHC1LLN"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from collections import deque \n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def nnModel(input_shape, action_space):\n",
        "\n",
        "  x_input = Input(input_shape)\n",
        "\n",
        "  # Dense is the basic form of a neural network layer\n",
        "  # Input layer of state size 4 and Hidden Layer with 512 nodes\n",
        "  x = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(x_input)\n",
        "\n",
        "  # Hidden Layer with 256 nodes\n",
        "  x = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
        "\n",
        "  # Hidden Layer with 64 nodes\n",
        "  x = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
        "\n",
        "  # Output Layer with # of actions: 2 nodes (left, right)\n",
        "  x = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(x)\n",
        "\n",
        "  model = Model(inputs= x_input, outputs = x, name='CartPoleDQNmodel')\n",
        "  model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "class DQNAgent():\n",
        "  def __init__(self):\n",
        "    self.env = gym.make('CartPole-v1')\n",
        "    # by default, CartPole-v1 has max episode steps = 500\n",
        "    self.state_size = self.env.observation_space.shape[0]\n",
        "    self.action_size = self.env.action_space.n\n",
        "    self.EPISODES = 1000\n",
        "    self.memory = deque(maxlen=2000)\n",
        "    \n",
        "    self.gamma = 0.95    # discount rate\n",
        "    self.epsilon = 1.0  # exploration rate\n",
        "    self.epsilon_min = 0.001\n",
        "    self.epsilon_decay = 0.999\n",
        "    self.batch_size = 64\n",
        "    self.train_start = 1000\n",
        "\n",
        "    # create main model\n",
        "    self.model = nnModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
        "\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((self, state, action, reward, next_state, done))\n",
        "    if len(self.memory) > self.train_start:\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "        # Discount epsilon to maximize the discounted future reward\n",
        "        self.epsilon *= self.epsilon_decay \n",
        "\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.random() <= self.epsilon:\n",
        "      return random.randrange(self.action_size)\n",
        "    else:\n",
        "      return np.argmax(self.model.predict(state))\n",
        "\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "\n",
        "  def replay(self):\n",
        "    if len(self.memory) < self.train_start:\n",
        "      return\n",
        "\n",
        "    # randomly sample minibatch from the memory\n",
        "    minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
        "    state = np.zeros((self.batch_size, self.state_size))\n",
        "    next_state = np.zeros((self.batch_size, self.state_size))\n",
        "    action, reward, done = [], [], []\n",
        "    \n",
        "    # this can be done using tensors for faster computational time\n",
        "    for i in range(self.batch_size):\n",
        "      state[i] = minibatch[i][1]\n",
        "      action.append(minibatch[i][2])\n",
        "      reward.append(minibatch[i][3])\n",
        "      next_state[i] = minibatch[i][4]\n",
        "      done.append(minibatch[i][5])\n",
        "\n",
        "    # do batch prediction to save speed\n",
        "    target = self.model.predict(state)\n",
        "    target_next = self.model.predict(next_state)\n",
        "\n",
        "    for i in range(self.batch_size):\n",
        "      if done[i]:\n",
        "        target[i][action[i]] = reward[i]\n",
        "      else:\n",
        "        # DQN chooses the max Q value among next actions\n",
        "        # Q_max = max_a' Q_target(s', a')\n",
        "        target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
        "        \n",
        "      # Train the Neural Network\n",
        "      self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
        "\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model = load_model(name)\n",
        "\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "\n",
        "  def save(self, name):\n",
        "    self.model.save(name)\n",
        "\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "\n",
        "  def run(self):\n",
        "    for e in range(self.EPISODES):\n",
        "      state = self.env.reset()\n",
        "      state = np.reshape(state, [1, self.state_size])\n",
        "      done = False\n",
        "      i = 0\n",
        "      while not done:\n",
        "        action = self.act(state)\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, self.state_size])\n",
        "       # if not done or i == 1000:\n",
        "       #   reward = reward\n",
        "       # else:\n",
        "       #   reward = -100\n",
        "        self.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        i += 1\n",
        "        if done:\n",
        "          print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES,i, self.epsilon))\n",
        "          if i == 500:\n",
        "            print(\"Saving trained model as cartpole-dqn.h5\")\n",
        "            self.save(\"cartpole-dqn.h5\")\n",
        "            return\n",
        "        self.replay()\n",
        "\n",
        "###############################################################################################\n",
        "###############################################################################################\n",
        "\n",
        "  def test(self):\n",
        "    self.load(\"cartpole-dqn.h5\")\n",
        "    for e in range(self.EPISODES):\n",
        "      state = self.env.reset()\n",
        "      state = np.reshape(state, [1, self.state_size])\n",
        "      done = False\n",
        "      i = 0\n",
        "      while not done:\n",
        "        #self.env.render()\n",
        "        action = np.argmax(self.model.predict(state))\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        state = np.reshape(next_state, [1, self.state_size])\n",
        "        i += 1\n",
        "        if done:\n",
        "          print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
        "          break\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpLleYrb4kYc",
        "outputId": "91d9172d-5dc3-4a9a-9051-c46fd3ded1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  agent = DQNAgent()\n",
        "  agent.run()\n",
        "  #agent.test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"CartPoleDQNmodel\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 512)               2560      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 150,466\n",
            "Trainable params: 150,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "episode: 0/1000, score: 31, e: 1.0\n",
            "episode: 1/1000, score: 15, e: 1.0\n",
            "episode: 2/1000, score: 19, e: 1.0\n",
            "episode: 3/1000, score: 14, e: 1.0\n",
            "episode: 4/1000, score: 17, e: 1.0\n",
            "episode: 5/1000, score: 42, e: 1.0\n",
            "episode: 6/1000, score: 16, e: 1.0\n",
            "episode: 7/1000, score: 14, e: 1.0\n",
            "episode: 8/1000, score: 41, e: 1.0\n",
            "episode: 9/1000, score: 17, e: 1.0\n",
            "episode: 10/1000, score: 17, e: 1.0\n",
            "episode: 11/1000, score: 20, e: 1.0\n",
            "episode: 12/1000, score: 17, e: 1.0\n",
            "episode: 13/1000, score: 11, e: 1.0\n",
            "episode: 14/1000, score: 64, e: 1.0\n",
            "episode: 15/1000, score: 63, e: 1.0\n",
            "episode: 16/1000, score: 12, e: 1.0\n",
            "episode: 17/1000, score: 16, e: 1.0\n",
            "episode: 18/1000, score: 35, e: 1.0\n",
            "episode: 19/1000, score: 14, e: 1.0\n",
            "episode: 20/1000, score: 23, e: 1.0\n",
            "episode: 21/1000, score: 38, e: 1.0\n",
            "episode: 22/1000, score: 16, e: 1.0\n",
            "episode: 23/1000, score: 66, e: 1.0\n",
            "episode: 24/1000, score: 27, e: 1.0\n",
            "episode: 25/1000, score: 25, e: 1.0\n",
            "episode: 26/1000, score: 22, e: 1.0\n",
            "episode: 27/1000, score: 14, e: 1.0\n",
            "episode: 28/1000, score: 23, e: 1.0\n",
            "episode: 29/1000, score: 12, e: 1.0\n",
            "episode: 30/1000, score: 19, e: 1.0\n",
            "episode: 31/1000, score: 23, e: 1.0\n",
            "episode: 32/1000, score: 25, e: 1.0\n",
            "episode: 33/1000, score: 18, e: 1.0\n",
            "episode: 34/1000, score: 18, e: 1.0\n",
            "episode: 35/1000, score: 10, e: 1.0\n",
            "episode: 36/1000, score: 18, e: 1.0\n",
            "episode: 37/1000, score: 18, e: 1.0\n",
            "episode: 38/1000, score: 20, e: 1.0\n",
            "episode: 39/1000, score: 16, e: 1.0\n",
            "episode: 40/1000, score: 13, e: 1.0\n",
            "episode: 41/1000, score: 34, e: 1.0\n",
            "episode: 42/1000, score: 12, e: 1.0\n",
            "episode: 43/1000, score: 21, e: 0.97\n",
            "episode: 44/1000, score: 9, e: 0.97\n",
            "episode: 45/1000, score: 21, e: 0.95\n",
            "episode: 46/1000, score: 60, e: 0.89\n",
            "episode: 47/1000, score: 16, e: 0.88\n",
            "episode: 48/1000, score: 11, e: 0.87\n",
            "episode: 49/1000, score: 16, e: 0.85\n",
            "episode: 50/1000, score: 28, e: 0.83\n",
            "episode: 51/1000, score: 49, e: 0.79\n",
            "episode: 52/1000, score: 15, e: 0.78\n",
            "episode: 53/1000, score: 21, e: 0.76\n",
            "episode: 54/1000, score: 9, e: 0.75\n",
            "episode: 55/1000, score: 13, e: 0.75\n",
            "episode: 56/1000, score: 27, e: 0.73\n",
            "episode: 57/1000, score: 11, e: 0.72\n",
            "episode: 58/1000, score: 48, e: 0.68\n",
            "episode: 59/1000, score: 68, e: 0.64\n",
            "episode: 60/1000, score: 72, e: 0.59\n",
            "episode: 61/1000, score: 13, e: 0.59\n",
            "episode: 62/1000, score: 27, e: 0.57\n",
            "episode: 63/1000, score: 22, e: 0.56\n",
            "episode: 64/1000, score: 23, e: 0.55\n",
            "episode: 65/1000, score: 12, e: 0.54\n",
            "episode: 66/1000, score: 21, e: 0.53\n",
            "episode: 67/1000, score: 16, e: 0.52\n",
            "episode: 68/1000, score: 15, e: 0.51\n",
            "episode: 69/1000, score: 56, e: 0.48\n",
            "episode: 70/1000, score: 33, e: 0.47\n",
            "episode: 71/1000, score: 98, e: 0.42\n",
            "episode: 72/1000, score: 17, e: 0.42\n",
            "episode: 73/1000, score: 65, e: 0.39\n",
            "episode: 74/1000, score: 53, e: 0.37\n",
            "episode: 75/1000, score: 24, e: 0.36\n",
            "episode: 76/1000, score: 76, e: 0.34\n",
            "episode: 77/1000, score: 52, e: 0.32\n",
            "episode: 78/1000, score: 66, e: 0.3\n",
            "episode: 79/1000, score: 110, e: 0.27\n",
            "episode: 80/1000, score: 15, e: 0.26\n",
            "episode: 81/1000, score: 32, e: 0.25\n",
            "episode: 82/1000, score: 48, e: 0.24\n",
            "episode: 83/1000, score: 134, e: 0.21\n",
            "episode: 84/1000, score: 117, e: 0.19\n",
            "episode: 85/1000, score: 14, e: 0.19\n",
            "episode: 86/1000, score: 103, e: 0.17\n",
            "episode: 87/1000, score: 109, e: 0.15\n",
            "episode: 88/1000, score: 37, e: 0.15\n",
            "episode: 89/1000, score: 22, e: 0.14\n",
            "episode: 90/1000, score: 116, e: 0.13\n",
            "episode: 91/1000, score: 134, e: 0.11\n",
            "episode: 92/1000, score: 46, e: 0.11\n",
            "episode: 93/1000, score: 89, e: 0.097\n",
            "episode: 94/1000, score: 175, e: 0.081\n",
            "episode: 95/1000, score: 122, e: 0.072\n",
            "episode: 96/1000, score: 105, e: 0.065\n",
            "episode: 97/1000, score: 117, e: 0.058\n",
            "episode: 98/1000, score: 189, e: 0.048\n",
            "episode: 99/1000, score: 150, e: 0.041\n",
            "episode: 100/1000, score: 120, e: 0.036\n",
            "episode: 101/1000, score: 155, e: 0.031\n",
            "episode: 102/1000, score: 103, e: 0.028\n",
            "episode: 103/1000, score: 114, e: 0.025\n",
            "episode: 104/1000, score: 272, e: 0.019\n",
            "episode: 105/1000, score: 267, e: 0.015\n",
            "episode: 106/1000, score: 42, e: 0.014\n",
            "episode: 107/1000, score: 16, e: 0.014\n",
            "episode: 108/1000, score: 14, e: 0.014\n",
            "episode: 109/1000, score: 169, e: 0.011\n",
            "episode: 110/1000, score: 175, e: 0.0096\n",
            "episode: 111/1000, score: 168, e: 0.0081\n",
            "episode: 112/1000, score: 154, e: 0.007\n",
            "episode: 113/1000, score: 117, e: 0.0062\n",
            "episode: 114/1000, score: 168, e: 0.0052\n",
            "episode: 115/1000, score: 220, e: 0.0042\n",
            "episode: 116/1000, score: 32, e: 0.0041\n",
            "episode: 117/1000, score: 228, e: 0.0032\n",
            "episode: 118/1000, score: 343, e: 0.0023\n",
            "episode: 119/1000, score: 205, e: 0.0019\n",
            "episode: 120/1000, score: 128, e: 0.0017\n",
            "episode: 121/1000, score: 140, e: 0.0014\n",
            "episode: 122/1000, score: 78, e: 0.0013\n",
            "episode: 123/1000, score: 177, e: 0.0011\n",
            "episode: 124/1000, score: 119, e: 0.001\n",
            "episode: 125/1000, score: 134, e: 0.001\n",
            "episode: 126/1000, score: 12, e: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}